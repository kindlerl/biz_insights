{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.8 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 5.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: 3ef00648-2f08-42f5-96b6-155dbdeaa765\nApplying the following default arguments:\n--glue_kernel_version 1.0.8\n--enable-glue-datacatalog true\nWaiting for session 3ef00648-2f08-42f5-96b6-155dbdeaa765 to get into ready status...\nSession 3ef00648-2f08-42f5-96b6-155dbdeaa765 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "from datetime import datetime\nfrom pyspark.sql import SparkSession, functions as F\n\nspark = SparkSession.builder.appName(\"bronze-date-dim-fix\").getOrCreate()\n\n# Avoid $folder$ marker writes if your IAM is strict\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.create.directory\", \"false\")\n\nBUCKET = \"bus-insights-dev-us-east-1\"\n\n# We want both partitions to mean \"business date\", i.e., ingestion_date == date_key\nFIX_DATES = [\"2025-09-27\", \"2024-02-22\"]  # exactly your two partitions\n\ndef rewrite_date_dim(day: str):\n    dt = datetime.strptime(day, \"%Y-%m-%d\")\n    df = spark.createDataFrame([{\n        \"date_key\": day,\n        \"year\": dt.year,\n        \"month\": dt.month,\n        \"day\": dt.day,\n        \"week\": int(dt.strftime(\"%V\")),       # ISO week\n        \"day_of_week\": dt.strftime(\"%A\"),\n        \"is_weekend\": dt.weekday() >= 5,\n        \"is_holiday\": False,\n        \"holiday_name\": None\n    }])\n    (df.withColumn(\"ingestion_date\", F.lit(day))\n       .write.mode(\"overwrite\")\n       .option(\"compression\",\"snappy\")\n       .parquet(f\"s3://{BUCKET}/bronze/date_dim/ingestion_date={day}/\"))\n    print(f\"Rewrote bronze/date_dim for {day}\")\n\nfor d in FIX_DATES:\n    rewrite_date_dim(d)\n",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "PySparkValueError: [CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from datetime import datetime\nfrom pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.types import (\n    StructType, StructField, StringType, IntegerType, BooleanType\n)\n\nspark = SparkSession.builder.appName(\"bronze-date-dim-fix\").getOrCreate()\n# Optional: avoid $folder$ markers if your IAM is strict\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.create.directory\", \"false\")\n\nBUCKET = \"bus-insights-dev-us-east-1\"\nFIX_DATES = [\"2025-09-27\", \"2024-02-22\"]  # your two partitions\n\n# Explicit schema (matches what we want in Bronze)\nschema = StructType([\n    StructField(\"date_key\",     StringType(),  False),\n    StructField(\"year\",         IntegerType(), False),\n    StructField(\"month\",        IntegerType(), False),\n    StructField(\"day\",          IntegerType(), False),\n    StructField(\"week\",         IntegerType(), False),\n    StructField(\"day_of_week\",  StringType(),  False),\n    StructField(\"is_weekend\",   BooleanType(), False),\n    StructField(\"is_holiday\",   BooleanType(), False),\n    StructField(\"holiday_name\", StringType(),  True),\n    # keep ingestion_date in-file as STRING to match your Bronze table metadata\n    StructField(\"ingestion_date\", StringType(), False),\n])\n\ndef rewrite_date_dim(day: str):\n    dt = datetime.strptime(day, \"%Y-%m-%d\")\n    rows = [{\n        \"date_key\": day,\n        \"year\": dt.year,\n        \"month\": dt.month,\n        \"day\": dt.day,\n        \"week\": int(dt.strftime(\"%V\")),        # ISO week number\n        \"day_of_week\": dt.strftime(\"%A\"),\n        \"is_weekend\": dt.weekday() >= 5,\n        \"is_holiday\": False,\n        \"holiday_name\": None,\n        \"ingestion_date\": day                  # keep semantics aligned\n    }]\n    df = spark.createDataFrame(rows, schema=schema)\n\n    out = f\"s3://{BUCKET}/bronze/date_dim/ingestion_date={day}/\"\n    (df.write.mode(\"overwrite\").option(\"compression\",\"snappy\").parquet(out))\n    print(f\"Rewrote bronze/date_dim for {day} -> {out}\")\n\nfor d in FIX_DATES:\n    rewrite_date_dim(d)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "Rewrote bronze/date_dim for 2025-09-27 -> s3://bus-insights-dev-us-east-1/bronze/date_dim/ingestion_date=2025-09-27/\nRewrote bronze/date_dim for 2024-02-22 -> s3://bus-insights-dev-us-east-1/bronze/date_dim/ingestion_date=2024-02-22/\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Read both partitions back and show schema/preview\nbase = f\"s3://{BUCKET}/bronze/date_dim/\"\ndf_all = spark.read.option(\"basePath\", base).parquet(base)\ndf_all.printSchema()\ndf_all.orderBy(\"ingestion_date\").show(5, False)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- date_key: string (nullable = true)\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- day: integer (nullable = true)\n |-- week: integer (nullable = true)\n |-- day_of_week: string (nullable = true)\n |-- is_weekend: boolean (nullable = true)\n |-- is_holiday: boolean (nullable = true)\n |-- holiday_name: string (nullable = true)\n |-- ingestion_date: date (nullable = true)\n\n+----------+----+-----+---+----+-----------+----------+----------+------------+--------------+\n|date_key  |year|month|day|week|day_of_week|is_weekend|is_holiday|holiday_name|ingestion_date|\n+----------+----+-----+---+----+-----------+----------+----------+------------+--------------+\n|2024-02-22|2024|2    |22 |8   |Thursday   |false     |false     |NULL        |2024-02-22    |\n|2025-09-27|2025|9    |27 |39  |Saturday   |true      |false     |NULL        |2025-09-27    |\n+----------+----+-----+---+----+-----------+----------+----------+------------+--------------+\n",
					"output_type": "stream"
				}
			]
		}
	]
}